{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96e75fd",
   "metadata": {
    "endofcell": "--"
   },
   "outputs": [],
   "source": [
    "# + metadata={}\n",
    "import sys, os\n",
    "\n",
    "sys.path += [\"../../external/\"]\n",
    "\n",
    "# + metadata={}\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport l4acados\n",
    "\n",
    "# + metadata={}\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import casadi as cas\n",
    "from acados_template import (\n",
    "    AcadosOcp,\n",
    "    AcadosSim,\n",
    "    AcadosSimSolver,\n",
    "    AcadosOcpSolver,\n",
    "    AcadosOcpOptions,\n",
    "    ZoroDescription,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "import copy\n",
    "\n",
    "# zoRO imports\n",
    "import l4acados\n",
    "from l4acados.controllers import (\n",
    "    ZeroOrderGPMPC,\n",
    ")\n",
    "from l4acados.controllers.zoro_acados_utils import setup_sim_from_ocp\n",
    "from inverted_pendulum_model_acados import (\n",
    "    export_simplependulum_ode_model,\n",
    "    export_ocp_nominal,\n",
    ")\n",
    "from utils import *\n",
    "\n",
    "# gpytorch_utils\n",
    "from gpytorch_utils.gp_hyperparam_training import (\n",
    "    generate_train_inputs_acados,\n",
    "    generate_train_outputs_at_inputs,\n",
    "    train_gp_model,\n",
    ")\n",
    "from gpytorch_utils.gp_utils import (\n",
    "    gp_data_from_model_and_path,\n",
    "    gp_derivative_data_from_model_and_path,\n",
    "    plot_gp_data,\n",
    "    generate_grid_points,\n",
    ")\n",
    "from l4acados.models.pytorch_models.gpytorch_models.gpytorch_gp import (\n",
    "    BatchIndependentMultitaskGPModel,\n",
    ")\n",
    "\n",
    "# -\n",
    "\n",
    "\n",
    "# ## Define model parameters\n",
    "#\n",
    "# We model the inverted pendulum\n",
    "#\n",
    "# $$\n",
    "# \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ \\ddot{\\theta} \\end{bmatrix} = \\begin{bmatrix} \\dot{\\theta} \\\\ -\\sin(\\theta) + u \\end{bmatrix},\n",
    "# $$\n",
    "#\n",
    "# which is to be controlled from the hanging-down resting position, $(\\theta_0, \\dot{\\theta}_0) = (\\pi, 0)$, to the upright position $(\\theta_r, \\dot{\\theta}_r) = (0,0)$, subject to the constraints that overshoot should be avoided, i.e.,\n",
    "#\n",
    "# $$\n",
    "# \\theta_{lb} \\leq \\theta \\leq \\theta_{ub}.\n",
    "# $$\n",
    "#\n",
    "# The model setup and controller definition can be found in the functions `export_simplependulum_ode_model()`, `export_ocp_nominal()` in the `inverted_pendulum_model_acados.py` file.\n",
    "\n",
    "# + metadata={}\n",
    "# build C code again?\n",
    "build_c_code = True\n",
    "\n",
    "# + metadata={}\n",
    "# discretization\n",
    "N = 30\n",
    "T = 5\n",
    "dT = T / N\n",
    "\n",
    "# constraints\n",
    "x0 = np.array([np.pi, 0])\n",
    "nx = 2\n",
    "nu = 1\n",
    "\n",
    "\n",
    "# + metadata={}\n",
    "prob_x = 0.95\n",
    "prob_tighten = norm.ppf(prob_x)\n",
    "\n",
    "# noise\n",
    "# uncertainty dynamics\n",
    "sigma_theta = (0.0001 / 360.0) * 2 * np.pi\n",
    "sigma_omega = (0.0001 / 360.0) * 2 * np.pi\n",
    "w_theta = 0.005\n",
    "w_omega = 0.005\n",
    "Sigma_x0 = np.array([[sigma_theta**2, 0], [0, sigma_omega**2]])\n",
    "Sigma_W = np.array([[w_theta**2, 0], [0, w_omega**2]])\n",
    "# -\n",
    "\n",
    "# ## Set up nominal solver\n",
    "\n",
    "# + metadata={}\n",
    "ocp_init = export_ocp_nominal(N, T, model_name=\"simplependulum_ode_init\")\n",
    "ocp_init.solver_options.nlp_solver_type = \"SQP\"\n",
    "\n",
    "# + metadata={}\n",
    "ocp_init.solver_options.Tsim\n",
    "\n",
    "# + metadata={}\n",
    "acados_ocp_init_solver = AcadosOcpSolver(\n",
    "    ocp_init, json_file=\"acados_ocp_init_simplependulum_ode.json\"\n",
    ")\n",
    "\n",
    "# + metadata={}\n",
    "ocp_init.solver_options.Tsim\n",
    "# -\n",
    "\n",
    "# ## Open-loop planning with nominal solver\n",
    "\n",
    "# + metadata={}\n",
    "X_init, U_init = get_solution(acados_ocp_init_solver, x0)\n",
    "\n",
    "# + metadata={}\n",
    "# integrator for nominal model\n",
    "sim = setup_sim_from_ocp(ocp_init)\n",
    "\n",
    "acados_integrator = AcadosSimSolver(\n",
    "    sim, json_file=\"acados_sim_\" + sim.model.name + \".json\"\n",
    ")\n",
    "# -\n",
    "\n",
    "# ## Simulator object\n",
    "#\n",
    "# To automatically discretize the model (and obtain sensitivities of the discrete-time model) within the zero-order implementation, we create the `AcadosSimSolver` object to pass to the solver.\n",
    "\n",
    "# + metadata={}\n",
    "# generate training data for GP with \"real model\"\n",
    "model_actual = export_simplependulum_ode_model(\n",
    "    model_name=sim.model.name + \"_actual\", add_residual_dynamics=True\n",
    ")\n",
    "\n",
    "sim_actual = setup_sim_from_ocp(ocp_init)\n",
    "sim_actual.model = model_actual\n",
    "\n",
    "# acados_ocp_solver = AcadosOcpSolver(ocp, json_file = 'acados_ocp_' + model.name + '.json')\n",
    "acados_integrator_actual = AcadosSimSolver(\n",
    "    sim_actual, json_file=\"acados_sim_\" + model_actual.name + \".json\"\n",
    ")\n",
    "# -\n",
    "\n",
    "# ## Simulation results (nominal)\n",
    "\n",
    "# + metadata={}\n",
    "X_init_sim = simulate_solution(acados_integrator_actual, x0, N, nx, nu, U_init)\n",
    "\n",
    "# + metadata={}\n",
    "lb_theta = -ocp_init.constraints.lh[0]\n",
    "fig, ax = base_plot(lb_theta=lb_theta)\n",
    "\n",
    "plot_data_nom = EllipsoidTubeData2D(center_data=X_init, ellipsoid_data=None)\n",
    "plot_data_nom_sim = EllipsoidTubeData2D(center_data=X_init_sim, ellipsoid_data=None)\n",
    "add_plot_trajectory(ax, plot_data_nom, prob_tighten=None, color_fun=plt.cm.Blues)\n",
    "add_plot_trajectory(ax, plot_data_nom_sim, prob_tighten=None, color_fun=plt.cm.Blues)\n",
    "# -\n",
    "\n",
    "# # GP training\n",
    "#\n",
    "# We use a model with different parameters to emulate the real-world model and obtain some training data. Also create simulator object for real-world model to evaluate our results later (not used in solver).\n",
    "\n",
    "# ## Generate training data\n",
    "#\n",
    "# We generate training data (one-step ahead residuals `y_train` for starting point `x_train`) here by running robustified (cautious) solver without GP.\n",
    "\n",
    "# + metadata={}\n",
    "random_seed = 123\n",
    "N_sim_per_x0 = 1\n",
    "N_x0 = 10\n",
    "x0_rand_scale = 0.1\n",
    "\n",
    "x_train, x0_arr = generate_train_inputs_acados(\n",
    "    acados_ocp_init_solver,\n",
    "    x0,\n",
    "    N_sim_per_x0,\n",
    "    N_x0,\n",
    "    random_seed=random_seed,\n",
    "    x0_rand_scale=x0_rand_scale,\n",
    ")\n",
    "\n",
    "y_train = generate_train_outputs_at_inputs(\n",
    "    x_train, acados_integrator, acados_integrator_actual, Sigma_W\n",
    ")\n",
    "\n",
    "# + metadata={}\n",
    "x_train\n",
    "# -\n",
    "\n",
    "# ## Hyper-parameter training for GP model\n",
    "#\n",
    "# Optimize hyper-parameters of GP model (kernel function parameters, ...)\n",
    "\n",
    "# + metadata={}\n",
    "x_train_tensor = torch.Tensor(x_train)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "nout = y_train.shape[1]\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=nout)\n",
    "gp_model = BatchIndependentMultitaskGPModel(x_train_tensor, y_train_tensor, likelihood)\n",
    "\n",
    "# + metadata={}\n",
    "load_gp_model_from_state_dict = False\n",
    "state_dict_path_gp_model = \"gp_model_state_dict.pth\"\n",
    "state_dict_path_likelihood = \"gp_model_likelihood_state_dict.pth\"\n",
    "train_data_path = \"gp_model_train_data.pth\"\n",
    "\n",
    "if load_gp_model_from_state_dict:\n",
    "    # Load state dict\n",
    "    gp_model.load_state_dict(torch.load(state_dict_path_gp_model))\n",
    "    likelihood.load_state_dict(torch.load(state_dict_path_likelihood))\n",
    "else:\n",
    "    training_iterations = 200\n",
    "    rng_seed = 456\n",
    "\n",
    "    gp_model, likelihood = train_gp_model(\n",
    "        gp_model, torch_seed=rng_seed, training_iterations=training_iterations\n",
    "    )\n",
    "\n",
    "# EVAL MODE\n",
    "gp_model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# + metadata={}\n",
    "# save GP hyper-params\n",
    "torch.save(gp_model.state_dict(), state_dict_path_gp_model)\n",
    "torch.save(likelihood.state_dict(), state_dict_path_likelihood)\n",
    "torch.save({\"x_train\": x_train_tensor, \"y_train\": y_train_tensor}, train_data_path)\n",
    "\n",
    "\n",
    "# + metadata={}\n",
    "data_dict = torch.load(train_data_path)\n",
    "data_dict\n",
    "# -\n",
    "\n",
    "# ## Plot GP predictions\n",
    "#\n",
    "# We plot GP predictions along the predicted trajectory of the robustified solver by projecting the multivariate plot down to a line.\n",
    "\n",
    "# + metadata={}\n",
    "x_train.shape, y_train.shape\n",
    "\n",
    "# + metadata={}\n",
    "num_samples = 5\n",
    "use_likelihood = False\n",
    "\n",
    "num_points_between_samples = 30\n",
    "t_lin = np.linspace(0, 1, num_points_between_samples, endpoint=False)\n",
    "\n",
    "x_plot_waypts = np.hstack((X_init[1:, :], U_init))\n",
    "x_plot = []\n",
    "for i in range(x_plot_waypts.shape[0] - 1):\n",
    "    x_plot += [\n",
    "        x_plot_waypts[i, :] + (x_plot_waypts[i + 1, :] - x_plot_waypts[i, :]) * t\n",
    "        for t in t_lin\n",
    "    ]\n",
    "x_plot = np.vstack(x_plot)\n",
    "\n",
    "gp_data = gp_data_from_model_and_path(\n",
    "    gp_model, likelihood, x_plot, num_samples=num_samples, use_likelihood=use_likelihood\n",
    ")\n",
    "plot_gp_data([gp_data], marker_size_lim=[1, 15])\n",
    "# -\n",
    "\n",
    "# We can also plot the derivative of the GP. Note that the projected Jacobian is not smooth since our path is not smooth either (jump projection direction = jump in Jacobian); however, the actual Jacobian should be smooth here (squared exponential kernel).\n",
    "\n",
    "# + metadata={}\n",
    "gp_derivative_data = gp_derivative_data_from_model_and_path(\n",
    "    gp_model, likelihood, x_plot, num_samples=0\n",
    ")\n",
    "plot_gp_data([gp_derivative_data], marker_size_lim=[5, 20], plot_train_data=False)\n",
    "# -\n",
    "\n",
    "# Compare with plotting along a slice of the dimension. Since we generated training data along the path of the robustified controller, the GP looks pretty untrained along a slice of the coordinates.\n",
    "\n",
    "# + metadata={}\n",
    "# plot along axis\n",
    "x_dim_lims = np.array([[0, np.pi], [-2, 1], [-2, 2]])\n",
    "x_dim_slice = np.array([1 * np.pi, 0, 0])\n",
    "x_dim_plot = 2\n",
    "x_grid = generate_grid_points(x_dim_lims, x_dim_slice, x_dim_plot, num_points=800)\n",
    "\n",
    "gp_grid_data = gp_data_from_model_and_path(\n",
    "    gp_model, likelihood, x_grid, num_samples=num_samples, use_likelihood=use_likelihood\n",
    ")\n",
    "fig, ax = plot_gp_data([gp_grid_data], marker_size_lim=[5, 50])\n",
    "\n",
    "y_lim_0 = ax[0].get_ylim()\n",
    "y_lim_1 = ax[1].get_ylim()\n",
    "# -\n",
    "\n",
    "# Jacobian... not much going on away from the data points (this is good!)\n",
    "\n",
    "# + metadata={}\n",
    "gp_derivative_grid_data = gp_derivative_data_from_model_and_path(\n",
    "    gp_model, likelihood, x_grid, num_samples=0\n",
    ")\n",
    "fig, ax = plot_gp_data(\n",
    "    [gp_derivative_grid_data], marker_size_lim=[5, 50], plot_train_data=False\n",
    ")\n",
    "\n",
    "ax[0].set_ylim(*y_lim_0)\n",
    "ax[1].set_ylim(*y_lim_1)\n",
    "plt.draw()\n",
    "# -\n",
    "\n",
    "# # Residual-Model MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5362e902",
   "metadata": {
    "endofcell": "--",
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from l4acados.models import GPyTorchResidualModel\n",
    "\n",
    "# + metadata={}\n",
    "residual_model = GPyTorchResidualModel(gp_model)\n",
    "\n",
    "# + metadata={}\n",
    "residual_model.evaluate(x_plot_waypts[0:3, :])\n",
    "\n",
    "# + metadata={}\n",
    "residual_model.jacobian(x_plot_waypts[0:3, :])\n",
    "\n",
    "# + metadata={}\n",
    "residual_model.value_and_jacobian(x_plot_waypts[0:3, :])\n",
    "# -\n",
    "\n",
    "# create zoro_description\n",
    "zoro_description = ZoroDescription()\n",
    "zoro_description.backoff_scaling_gamma = norm.ppf(prob_x)\n",
    "zoro_description.P0_mat = Sigma_x0\n",
    "zoro_description.fdbk_K_mat = np.zeros((nu, nx))\n",
    "# zoro_description.unc_jac_G_mat = B\n",
    "\"\"\"G in (nx, nw) describes how noise affects dynamics. I.e. x+ = ... + G@w\"\"\"\n",
    "zoro_description.W_mat = Sigma_W\n",
    "\"\"\"W in (nw, nw) describes the covariance of the noise on the system\"\"\"\n",
    "zoro_description.input_P0_diag = True\n",
    "zoro_description.input_P0 = False\n",
    "zoro_description.input_W_diag = True\n",
    "zoro_description.input_W_add_diag = True\n",
    "zoro_description.output_P_matrices = True\n",
    "zoro_description.idx_lh_t = [0]\n",
    "zoro_description.idx_lh_e_t = [0]\n",
    "ocp_init.zoro_description = zoro_description\n",
    "\n",
    "# + metadata={}\n",
    "residual_mpc = ZeroOrderGPMPC(\n",
    "    ocp_init,\n",
    "    residual_model=residual_model,\n",
    "    use_cython=False,\n",
    "    path_json_ocp=\"residual_mpc_ocp_solver_config.json\",\n",
    "    path_json_sim=\"residual_mpc_sim_solver_config.json\",\n",
    "    build_c_code=True,\n",
    ")\n",
    "\n",
    "# + metadata={}\n",
    "for i in range(N):\n",
    "    residual_mpc.ocp_solver.set(i, \"x\", X_init[i, :])\n",
    "    residual_mpc.ocp_solver.set(i, \"u\", U_init[i, :])\n",
    "residual_mpc.ocp_solver.set(N, \"x\", X_init[N, :])\n",
    "\n",
    "residual_mpc.solve()\n",
    "X_res, U_res = residual_mpc.get_solution()\n",
    "P_res_arr = residual_mpc.covariances_array\n",
    "\n",
    "P_res = []\n",
    "for i in range(N + 1):\n",
    "    P_res.append(P_res_arr[i * nx**2 : (i + 1) * nx**2].reshape((nx, nx)))\n",
    "P_res = np.array(P_res)\n",
    "\n",
    "# + metadata={}\n",
    "X_res_sim = np.zeros_like(X_res)\n",
    "X_res_sim[0, :] = x0\n",
    "for i in range(N):\n",
    "    acados_integrator_actual.set(\"x\", X_res_sim[i, :])\n",
    "    acados_integrator_actual.set(\"u\", U_res[i, :])\n",
    "    acados_integrator_actual.solve()\n",
    "    X_res_sim[i + 1, :] = acados_integrator_actual.get(\"x\")\n",
    "\n",
    "# + metadata={}\n",
    "lb_theta = -ocp_init.constraints.lh[0]\n",
    "fig, ax = base_plot(lb_theta=lb_theta)\n",
    "\n",
    "plot_data_res = EllipsoidTubeData2D(center_data=X_res, ellipsoid_data=P_res)\n",
    "plot_data_res_sim = EllipsoidTubeData2D(center_data=X_res_sim, ellipsoid_data=None)\n",
    "add_plot_trajectory(ax, plot_data_nom, color_fun=plt.cm.Blues)\n",
    "add_plot_trajectory(ax, plot_data_nom_sim, color_fun=plt.cm.Blues)\n",
    "add_plot_trajectory(\n",
    "    ax, plot_data_res, prob_tighten=prob_tighten, color_fun=plt.cm.Oranges\n",
    ")\n",
    "add_plot_trajectory(ax, plot_data_res_sim, color_fun=plt.cm.Oranges)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "l4acados_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
